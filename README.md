# Segwise_PlaystoreAppInsights

Data_Engineering Assignment for Segwise

# To run spark job "segwise_spark_job.py" file should be used

# Running a PySpark Job

## Prerequisites

- Apache Spark installed on your system.
- Python 3.9 or later version installed.
- Ensure that the necessary environment variables such as `SPARK_HOME` are properly set.

## Setup Apache Spark

1. Download Apache Spark from the [official website](https://spark.apache.org/downloads.html).
2. Extract the downloaded archive to a directory of your choice.

## Configure Environment Variables

1. Add the Spark binary directory to your system's `PATH` environment variable.
   ```bash
   export PATH=$PATH:/path/to/spark/bin
   ```

## Command to execute the job

spark-submit --master local segwise_spark_job.py

# OR It can be executed as python script by following these steps:

## First Ensure pyspark is installed, if not use the command below to install:

pip install pyspark

## After installing pyspark execute the command below

python3 <path_to-segwise_spark_job.py>

# To execute the ipynb notebook

#### Note: Prefered to use Jupyter Notebook, Google Colab etc.

## Requirements

- Python 3.x
- Apache Spark
- PySpark library

## Dataset

- File should be named as "playstore.csv".
- Use import os to check the current working directory use command: os.getcwd().
- Place dataset file on the location shown in the output.

## Output

- CSV generation will take time due to large dataset
- It will generated by the name "output_all_insights.csv" in the current working directory.
- CSV report will have data formatted as mentioned in assignment: Price=[4-5]; genre=Art & Design; Installs=[10000-100000], 100. (Note: code console output format will be different)

## Setup Instructions

### Install Apache Spark

1. Download Apache Spark from the [official website](https://spark.apache.org/downloads.html).
2. Extract the downloaded file to a preferred location on your machine.

### Install PySpark Library

Install PySpark using `pip`:

```bash
pip install pyspark


```
